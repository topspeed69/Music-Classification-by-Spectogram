{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d9490fd",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cc4e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a1d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8963826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository or upload project files\n",
    "# Option 1: Clone from GitHub\n",
    "!git clone https://github.com/topspeed69/Music-Classification-by-Spectogram.git\n",
    "%cd Music-Classification-by-Spectogram\n",
    "\n",
    "# Option 2: If files are in Google Drive\n",
    "# import os\n",
    "# os.chdir('/content/drive/MyDrive/Music-Classification-by-Spectogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf2484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Additional Colab-specific installations\n",
    "!pip install -q tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddbab58",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d457a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download or prepare dataset\n",
    "# Example: Download FMA dataset or use your own\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if data already exists\n",
    "data_dir = Path('AudioToSpectogram/fma_small_dataset')\n",
    "output_dir = Path('AudioToSpectogram/output')\n",
    "\n",
    "if not data_dir.exists():\n",
    "    print(\"Data directory not found. Please upload your audio files or download the FMA dataset.\")\n",
    "    print(\"You can download FMA small dataset from: https://github.com/mdeff/fma\")\n",
    "else:\n",
    "    print(f\"Found data directory: {data_dir}\")\n",
    "    audio_files = list(data_dir.rglob('*.mp3')) + list(data_dir.rglob('*.wav'))\n",
    "    print(f\"Number of audio files: {len(audio_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8589cfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate spectrograms from audio files (if not already done)\n",
    "import sys\n",
    "sys.path.append('AudioToSpectogram')\n",
    "\n",
    "from audio_to_spectogram_mel import AudioToMelSpectrogram\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "input_dir = 'AudioToSpectogram/fma_small_dataset'\n",
    "output_dir = 'AudioToSpectogram/output_mel'\n",
    "\n",
    "# Check if spectrograms already exist\n",
    "output_path = Path(output_dir)\n",
    "existing_spectrograms = list(output_path.rglob('*.png')) if output_path.exists() else []\n",
    "\n",
    "if len(existing_spectrograms) > 0:\n",
    "    print(f\"Found {len(existing_spectrograms)} existing spectrograms. Skipping generation.\")\n",
    "    print(\"Set regenerate=True to generate spectrograms again.\")\n",
    "    regenerate = False\n",
    "else:\n",
    "    regenerate = True\n",
    "\n",
    "if regenerate:\n",
    "    print(\"Generating mel spectrograms...\")\n",
    "    converter = AudioToMelSpectrogram(\n",
    "        input_dir=input_dir,\n",
    "        output_dir=output_dir,\n",
    "        sr=22050,\n",
    "        n_mels=128,\n",
    "        duration=3.0  # 3-second segments\n",
    "    )\n",
    "    converter.process_all_audio()\n",
    "    print(\"Spectrogram generation complete!\")\n",
    "\n",
    "# Count spectrograms\n",
    "spectrograms = list(Path(output_dir).rglob('*.png'))\n",
    "print(f\"\\nTotal spectrograms available: {len(spectrograms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2e8e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_train_val_split(source_dir, train_dir, val_dir, val_split=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Split spectrograms into train and validation sets\n",
    "    \"\"\"\n",
    "    source_path = Path(source_dir)\n",
    "    train_path = Path(train_dir)\n",
    "    val_path = Path(val_dir)\n",
    "    \n",
    "    # Create directories\n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    val_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get all spectrogram files\n",
    "    all_files = list(source_path.rglob('*.png'))\n",
    "    \n",
    "    # Split\n",
    "    train_files, val_files = train_test_split(\n",
    "        all_files, test_size=val_split, random_state=seed\n",
    "    )\n",
    "    \n",
    "    print(f\"Copying {len(train_files)} files to train...\")\n",
    "    for file in tqdm(train_files):\n",
    "        rel_path = file.relative_to(source_path)\n",
    "        dest = train_path / rel_path\n",
    "        dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if not dest.exists():\n",
    "            shutil.copy2(file, dest)\n",
    "    \n",
    "    print(f\"Copying {len(val_files)} files to validation...\")\n",
    "    for file in tqdm(val_files):\n",
    "        rel_path = file.relative_to(source_path)\n",
    "        dest = val_path / rel_path\n",
    "        dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if not dest.exists():\n",
    "            shutil.copy2(file, dest)\n",
    "    \n",
    "    return len(train_files), len(val_files)\n",
    "\n",
    "# Create split\n",
    "source_dir = 'AudioToSpectogram/output_mel'\n",
    "train_dir = 'data/train'\n",
    "val_dir = 'data/val'\n",
    "\n",
    "# Check if split already exists\n",
    "if Path(train_dir).exists() and Path(val_dir).exists():\n",
    "    train_count = len(list(Path(train_dir).rglob('*.png')))\n",
    "    val_count = len(list(Path(val_dir).rglob('*.png')))\n",
    "    print(f\"Train/val split already exists: {train_count} train, {val_count} val\")\n",
    "else:\n",
    "    train_count, val_count = create_train_val_split(source_dir, train_dir, val_dir, val_split=0.2)\n",
    "    print(f\"\\nSplit complete: {train_count} train, {val_count} val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a72c1e",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee470943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration files\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "def load_config(config_path):\n",
    "    \"\"\"Load YAML configuration file\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "# Load all configs\n",
    "model_config = load_config('configs/model_config.yaml')\n",
    "training_config = load_config('configs/training_config.yaml')\n",
    "data_config = load_config('configs/data_config.yaml')\n",
    "\n",
    "# Merge configs\n",
    "config = {**model_config, **training_config, **data_config}\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"\\nModel: {config['model']['name']}\")\n",
    "print(f\"Embedding dimension: {config['model']['projection_head']['embedding_dim']}\")\n",
    "print(f\"Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"Epochs: {config['training']['epochs']}\")\n",
    "print(f\"Learning rate: {config['training']['optimizer']['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a05171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update data paths for Colab\n",
    "config['data']['train_dir'] = 'data/train'\n",
    "config['data']['val_dir'] = 'data/val'\n",
    "\n",
    "# Adjust batch size for Colab GPU memory (if needed)\n",
    "# Reduce if you encounter OOM errors\n",
    "config['training']['batch_size'] = 32\n",
    "\n",
    "# Adjust number of workers for Colab\n",
    "config['training']['num_workers'] = 2\n",
    "\n",
    "print(\"Configuration updated for Colab environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a126b2a5",
   "metadata": {},
   "source": [
    "## 4. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8b8e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model components\n",
    "import sys\n",
    "sys.path.append('CNN')\n",
    "\n",
    "from CNN.models import build_model\n",
    "from CNN.augmentation import get_augmentation_pipeline\n",
    "from CNN.data import create_dataloaders\n",
    "from CNN.training import get_contrastive_loss\n",
    "from CNN.utils.metrics import AverageMeter\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Build model\n",
    "print(\"\\nBuilding model...\")\n",
    "model = build_model(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / 1e6:.2f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48cac45",
   "metadata": {},
   "source": [
    "## 5. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab46f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create augmentation pipelines\n",
    "train_transform = get_augmentation_pipeline(config, training=True)\n",
    "val_transform = get_augmentation_pipeline(config, training=False)\n",
    "\n",
    "# Create dataloaders\n",
    "print(\"Creating dataloaders...\")\n",
    "train_loader, val_loader = create_dataloaders(config, train_transform, val_transform)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Train samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Val samples: {len(val_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b45600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a batch of spectrograms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_batch(loader, num_samples=4):\n",
    "    \"\"\"\n",
    "    Visualize augmented spectrogram pairs\n",
    "    \"\"\"\n",
    "    view1, view2 = next(iter(loader))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(15, 6))\n",
    "    fig.suptitle('Augmented Spectrogram Pairs (View 1 and View 2)', fontsize=14)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # View 1\n",
    "        img1 = view1[i].cpu().numpy().transpose(1, 2, 0)\n",
    "        axes[0, i].imshow(img1)\n",
    "        axes[0, i].axis('off')\n",
    "        axes[0, i].set_title(f'View 1 - Sample {i+1}')\n",
    "        \n",
    "        # View 2\n",
    "        img2 = view2[i].cpu().numpy().transpose(1, 2, 0)\n",
    "        axes[1, i].imshow(img2)\n",
    "        axes[1, i].axis('off')\n",
    "        axes[1, i].set_title(f'View 2 - Sample {i+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualizing augmented pairs...\")\n",
    "visualize_batch(train_loader, num_samples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f33797e",
   "metadata": {},
   "source": [
    "## 6. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "\n",
    "# Create loss function\n",
    "contrastive_config = training_config['training']['contrastive']\n",
    "criterion = get_contrastive_loss(\n",
    "    loss_type=contrastive_config['loss_type'],\n",
    "    temperature=contrastive_config['temperature'],\n",
    "    use_cosine_similarity=contrastive_config['use_cosine_similarity']\n",
    ")\n",
    "\n",
    "# Create optimizer\n",
    "optimizer_config = training_config['training']['optimizer']\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=optimizer_config['learning_rate'],\n",
    "    weight_decay=optimizer_config['weight_decay']\n",
    ")\n",
    "\n",
    "# Create learning rate scheduler\n",
    "scheduler_config = training_config['training']['scheduler']\n",
    "if scheduler_config['type'] == 'cosine':\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=training_config['training']['epochs'],\n",
    "        eta_min=scheduler_config['min_lr']\n",
    "    )\n",
    "\n",
    "# Create checkpoint directory\n",
    "checkpoint_dir = Path('checkpoints')\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Training setup complete!\")\n",
    "print(f\"Optimizer: {optimizer_config['type']}\")\n",
    "print(f\"Learning rate: {optimizer_config['learning_rate']}\")\n",
    "print(f\"Loss function: {contrastive_config['loss_type']}\")\n",
    "print(f\"Temperature: {contrastive_config['temperature']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa6b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard\n",
    "%load_ext tensorboard\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "# Create log directory with timestamp\n",
    "log_dir = Path('runs') / datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "print(f\"TensorBoard log directory: {log_dir}\")\n",
    "print(\"Launch TensorBoard with: %tensorboard --logdir runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbfc50a",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5961cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for batch_idx, (view1, view2) in enumerate(pbar):\n",
    "        view1 = view1.to(device)\n",
    "        view2 = view2.to(device)\n",
    "        \n",
    "        # Forward pass for both views\n",
    "        z1 = model(view1)\n",
    "        z2 = model(view2)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(z1, z2)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        losses.update(loss.item(), view1.size(0))\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': f'{losses.avg:.4f}'})\n",
    "    \n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for view1, view2 in tqdm(val_loader, desc=\"Validation\"):\n",
    "            view1 = view1.to(device)\n",
    "            view2 = view2.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            z1 = model(view1)\n",
    "            z2 = model(view2)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(z1, z2)\n",
    "            losses.update(loss.item(), view1.size(0))\n",
    "    \n",
    "    return losses.avg\n",
    "\n",
    "print(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c1033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "import time\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "best_val_loss = float('inf')\n",
    "epochs = training_config['training']['epochs']\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Log to tensorboard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    writer.add_scalar('LR', current_lr, epoch)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch}/{epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        checkpoint_path = checkpoint_dir / 'best_model.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': config\n",
    "        }, checkpoint_path)\n",
    "        print(f\"✓ Saved best model (val_loss: {val_loss:.4f})\")\n",
    "    \n",
    "    # Save periodic checkpoint\n",
    "    if epoch % 10 == 0:\n",
    "        checkpoint_path = checkpoint_dir / f'checkpoint_epoch_{epoch}.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': config\n",
    "        }, checkpoint_path)\n",
    "        print(f\"✓ Saved checkpoint at epoch {epoch}\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# Training complete\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training completed!\")\n",
    "print(f\"Total time: {total_time/3600:.2f} hours\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495957a0",
   "metadata": {},
   "source": [
    "## 8. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee51e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "plt.plot(val_losses, label='Val Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss difference\n",
    "plt.subplot(1, 2, 2)\n",
    "loss_diff = [v - t for t, v in zip(train_losses, val_losses)]\n",
    "plt.plot(loss_diff, label='Val - Train', linewidth=2, color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Difference')\n",
    "plt.title('Overfitting Monitor')\n",
    "plt.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final val loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"Best val loss: {min(val_losses):.4f} (epoch {val_losses.index(min(val_losses))+1})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f761c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ebbfb0",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a0da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint_path = checkpoint_dir / 'best_model.pth'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "print(f\"Validation loss: {checkpoint['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49795afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings for a few samples\n",
    "from CNN.embeddings import extract_embeddings\n",
    "\n",
    "# Extract embeddings from validation set\n",
    "print(\"Extracting embeddings...\")\n",
    "embeddings, file_paths = extract_embeddings(\n",
    "    model=model,\n",
    "    data_dir=config['data']['val_dir'],\n",
    "    device=device,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "print(f\"\\nExtracted embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Number of samples: {len(file_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73022f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings using t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"Computing t-SNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.6, s=50)\n",
    "plt.title('t-SNE Visualization of Learned Embeddings', fontsize=14)\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('embeddings_tsne.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Embeddings visualization saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df8676",
   "metadata": {},
   "source": [
    "## 10. Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c8e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for inference\n",
    "export_dir = Path('exported_models')\n",
    "export_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save full model\n",
    "model_path = export_dir / 'music_encoder.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config,\n",
    "    'best_val_loss': best_val_loss\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model exported to: {model_path}\")\n",
    "print(f\"Model size: {model_path.stat().st_size / 1e6:.2f} MB\")\n",
    "\n",
    "# Save to Google Drive (optional)\n",
    "# import shutil\n",
    "# drive_path = '/content/drive/MyDrive/models/music_encoder.pth'\n",
    "# shutil.copy(model_path, drive_path)\n",
    "# print(f\"Model also saved to Google Drive: {drive_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download trained model and artifacts\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "# Create zip file with all artifacts\n",
    "zip_path = 'music_classification_artifacts.zip'\n",
    "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "    # Add model\n",
    "    zipf.write(model_path, 'music_encoder.pth')\n",
    "    \n",
    "    # Add training curves\n",
    "    if Path('training_curves.png').exists():\n",
    "        zipf.write('training_curves.png')\n",
    "    \n",
    "    # Add t-SNE visualization\n",
    "    if Path('embeddings_tsne.png').exists():\n",
    "        zipf.write('embeddings_tsne.png')\n",
    "    \n",
    "    # Add best checkpoint\n",
    "    if (checkpoint_dir / 'best_model.pth').exists():\n",
    "        zipf.write(checkpoint_dir / 'best_model.pth', 'best_model.pth')\n",
    "\n",
    "print(f\"Created artifact package: {zip_path}\")\n",
    "print(f\"Package size: {Path(zip_path).stat().st_size / 1e6:.2f} MB\")\n",
    "\n",
    "# Download\n",
    "# files.download(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e515bc99",
   "metadata": {},
   "source": [
    "## 11. Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2b18eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference on a single spectrogram\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def predict_embedding(model, image_path, device):\n",
    "    \"\"\"\n",
    "    Extract embedding for a single spectrogram\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Extract embedding\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image_tensor)\n",
    "    \n",
    "    return embedding.cpu().numpy()\n",
    "\n",
    "# Test on a sample\n",
    "sample_path = list(Path(config['data']['val_dir']).rglob('*.png'))[0]\n",
    "embedding = predict_embedding(model, sample_path, device)\n",
    "\n",
    "print(f\"Sample: {sample_path.name}\")\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"Embedding (first 10 values): {embedding[0, :10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07771c7a",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "### Files Generated:\n",
    "- `checkpoints/best_model.pth` - Best model checkpoint\n",
    "- `exported_models/music_encoder.pth` - Exported model for inference\n",
    "- `training_curves.png` - Training visualization\n",
    "- `embeddings_tsne.png` - Embedding visualization\n",
    "\n",
    "### To use this model:\n",
    "```python\n",
    "# Load model\n",
    "checkpoint = torch.load('exported_models/music_encoder.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Extract embeddings\n",
    "embedding = model(spectrogram_tensor)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
